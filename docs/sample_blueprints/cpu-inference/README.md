# CPU Inference Blueprint

This blueprint provides a comprehensive framework for testing inference on CPUs using the Ollama platform with a variety of supported models such as Mistral, Gemma, and others available through Ollama. Unlike GPU-dependent solutions, this blueprint is designed for environments where CPU inference is preferred or required. It offers clear guidelines and configuration settings to deploy a robust CPU inference service, enabling thorough performance evaluations and reliability testing. Ollama's lightweight and efficient architecture makes it an ideal solution for developers looking to benchmark and optimize CPU-based inference workloads.

## Pre-Filled Samples
| Title | Description|
|--------------------------------------------------|--------------------------------------------------------------------------------------------------------|
|CPU inference with Mistral and BM.Standard.E4     |Deploys CPU inference with Mistral and BM.Standard.E4 on BM.Standard.E4.128 with undefined GPU(s).      |
|CPU inference with Gemma and BM.Standard.E5.192   |Deploys CPU inference with Gemma and BM.Standard.E5.192 on BM.Standard.E5.192 with undefined GPU(s).    |
|CPU inference with mistral and VM.Standard.E4.Flex|Deploys CPU inference with mistral and VM.Standard.E4.Flex on VM.Standard.E4.Flex with undefined GPU(s).|
